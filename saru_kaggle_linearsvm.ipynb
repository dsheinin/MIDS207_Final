{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/local/bin/python\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "#import sklearn.ensemble\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import SelectPercentile,f_classif\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "import string #use the punctuation\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True) #Use Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of stop words and adjectives that cause overfitting \n",
    "cachedStopWords = stopwords.words(\"english\") # from nltk\n",
    "adj_words = ['refrigerated','fresh','freshly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (39774,)\n",
      "Dev_test data shape:  (0,)\n"
     ]
    }
   ],
   "source": [
    "# Open the Training Data file and divide into training and dev set\n",
    "train_file = os.path.join(\".\",\"train.json\")\n",
    "\n",
    "with open(train_file) as data_file:\n",
    "    train_data = json.loads(data_file.read())\n",
    "\n",
    "random.shuffle(train_data)\n",
    "\n",
    "n = 340000 #number of partitions, want all the training data so chose very high n... lazy approach I know\n",
    "dev_test_label = [d[\"cuisine\"] for d in train_data[:len(train_data)/n]]\n",
    "dev_train_label = [d[\"cuisine\"] for d in train_data[len(train_data)/n:]]\n",
    "\n",
    "# Preliminary Text Pre-Processing , rest is done in tokenize() of TF-IDF\n",
    "# data should be a list of strings\n",
    "# for tokenizer to token phrases instead of words, each line in an ingredient should be followed by a comma\n",
    "dev_test_data = []\n",
    "for doc in train_data[:len(train_data)/n]:\n",
    "    # collect all the words for that recipe\n",
    "    ingredient_phrase_list = []\n",
    "    for ingredient_phrase in doc[\"ingredients\"]: #this is a line/row\n",
    "        #lower case\n",
    "        ingr_phrase = ingredient_phrase.lower()\n",
    "        # strip digits from the word phrase\n",
    "        ingr_phrase = ' '.join([word for word in ingr_phrase.split() if not word.isdigit()])\n",
    "        # remove stop words from the phrase\n",
    "        ingr_phrase = ' '.join([word for word in ingr_phrase.split() if word not in cachedStopWords])\n",
    "        # remove other words from the phrase\n",
    "        ingr_phrase = ' '.join([word for word in ingr_phrase.split() if word not in adj_words])\n",
    "        # Append the ingredient phrase to join by ,\n",
    "        ingredient_phrase_list.append(ingr_phrase)\n",
    "     \n",
    "    #join each list item with ,\n",
    "    dev_test_data.append(','.join(ingredient_phrase_list))\n",
    "    \n",
    "dev_train_data = []\n",
    "for doc in train_data[len(train_data)/n:]:\n",
    "    # collect all the words for that recipe\n",
    "    ingredient_phrase_list = []\n",
    "    for ingredient_phrase in doc[\"ingredients\"]:\n",
    "        #lower case\n",
    "        ingr_phrase = ingredient_phrase.lower()\n",
    "        # strip digits from the word phrase\n",
    "        ingr_phrase = ' '.join([word for word in ingr_phrase.split() if not word.isdigit()])\n",
    "        # remove stop words from the phrase\n",
    "        ingr_phrase = ' '.join([word for word in ingr_phrase.split() if word not in cachedStopWords])\n",
    "        # remove other words from the phrase\n",
    "        ingr_phrase = ' '.join([word for word in ingr_phrase.split() if word not in adj_words])\n",
    "        # Append the ingredient phrase to join by ,\n",
    "        ingredient_phrase_list.append(ingr_phrase)\n",
    "     \n",
    "    #join each list item with ,\n",
    "    dev_train_data.append(','.join(ingredient_phrase_list))\n",
    "\n",
    "    \n",
    "print \"Train data shape: \",np.shape(dev_train_data)\n",
    "print \"Dev_test data shape: \",np.shape(dev_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all the Train Data and will do Cross validation on random samples of this training data.. you will see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape:  (9944,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Test Data in same way as Train data, except no partitioning\n",
    "# Open the Test Data file and divide into training and dev set\n",
    "test_file = os.path.join(\".\",\"test.json\")\n",
    "\n",
    "with open(test_file) as data_file:\n",
    "    testdata = json.loads(data_file.read())\n",
    "\n",
    "test_data = []\n",
    "test_ids = []\n",
    "for doc in testdata:\n",
    "    # collect the id for each test_data\n",
    "    test_ids.append(doc[\"id\"])\n",
    "    # collect all the words for that recipe\n",
    "    ingredient_phrase_list = []\n",
    "    for ingredient_phrase in doc[\"ingredients\"]: #this is a line/row\n",
    "        #lower case\n",
    "        ingr_phrase = ingredient_phrase.lower()\n",
    "        # strip digits from the word phrase\n",
    "        ingr_phrase = ' '.join([word for word in ingr_phrase.split() if not word.isdigit()])\n",
    "        # remove stop words from the phrase\n",
    "        ingr_phrase = ' '.join([word for word in ingr_phrase.split() if word not in cachedStopWords])\n",
    "        # remove other words from the phrase\n",
    "        ingr_phrase = ' '.join([word for word in ingr_phrase.split() if word not in adj_words])\n",
    "        # Append the ingredient phrase to join by ,\n",
    "        ingredient_phrase_list.append(ingr_phrase)\n",
    "     \n",
    "    #join each list item with ,\n",
    "    test_data.append(','.join(ingredient_phrase_list))\n",
    "\n",
    "print \"Test data shape: \",np.shape(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'grate',\n",
       " u'slice',\n",
       " u'chees',\n",
       " u'bake',\n",
       " u'powder',\n",
       " u'milk',\n",
       " u'extra',\n",
       " u'virgin',\n",
       " u'oliv',\n",
       " u'oil']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#units = ['and','or','warm','large','ground','whole', 'mince', 'shred','grate', 'slice','kosher']\n",
    "units = ['oz','and','or'] # after stemming remove these stop words\n",
    "#Change compound words after stemming to a simple word for standardization, this seems to reduce my score :(\n",
    "replacements = {\n",
    "                #'all purpos flour': 'flour',\n",
    "                #'extra virgin':'',\n",
    "                #'oliv oil': 'oliveoil',\n",
    "                }\n",
    "#The TFIDF vector tokenizes each string by comma\n",
    "def tokenize(text):\n",
    "    #print \"text: \", text\n",
    "    final_tokens = []\n",
    "    tokens = [ingredient for ingredient in text.split(',')]\n",
    "    # dont include empty tokens\n",
    "    for phrase in tokens:  \n",
    "        cleaned_phrase = phrase \n",
    "        if \"hidden valley\" in cleaned_phrase:\n",
    "               cleaned_phrase= \"hiddenvalley\"\n",
    "\n",
    "        # Replace unicode chars with letter c\n",
    "        cleaned_phrase = re.sub(r'[^\\x00-\\x7F]+','c', cleaned_phrase)\n",
    "        # Remove anything that is not alphabet\n",
    "        cleaned_phrase = re.sub('[^a-z]+', ' ', cleaned_phrase) \n",
    "        # Stem each word\n",
    "        cleaned_phrase = (\" \").join([stemmer.stem(word) for word in cleaned_phrase.split()])\n",
    "        #cleaned_phrase = (\" \").join([WordNetLemmatizer().lemmatize(word) for word in cleaned_phrase.split()])\n",
    "        # Remove any units from words in the phrase\n",
    "        cleaned_phrase = (\" \").join([word for word in cleaned_phrase.split() if word not in units])\n",
    "        # remove any unneccary white space that is in front of the phrase\n",
    "        cleaned_phrase = \" \".join(cleaned_phrase.split())\n",
    "        #print cleaned_phrase\n",
    "        \n",
    "        for key, value in replacements.items(): \n",
    "            cleaned_phrase = cleaned_phrase.replace(key,value)\n",
    "           \n",
    "        # flatten phrase\n",
    "        for word in cleaned_phrase.split():\n",
    "            #Replace words with correct spelling if spelt with alternative way\n",
    "            word= word.replace(\"yoghurt\",\"yogurt\")     \n",
    "            \n",
    "            if len(word) >1: #dont count single letter words\n",
    "                final_tokens.append(word)\n",
    "    \n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "tokenize('grates sliced cheese, baking-powder milk,   extra-  virgin olive    oil,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  74715\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform all the train data- these are tested params from grid search\n",
    "vec = TfidfVectorizer(tokenizer=tokenize, max_df=.50, min_df=1, lowercase=False,ngram_range=(1,2), binary=True,\n",
    "                     sublinear_tf=True)\n",
    "\n",
    "train = vec.fit_transform(dev_train_data)\n",
    "size =len(vec.get_feature_names())\n",
    "print \"Number of features: \", size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Selection using chi square- find and keep p% best correlation between words(features)\n",
    "ch2 = SelectPercentile(chi2, percentile=95) #use 95% of those features\n",
    "X_train_features = ch2.fit_transform(train, dev_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit into One vs Rest model( creates a model for each class and compares the confidence score\n",
    "# among the models for each data. Classifier choses is LinearSVM model\n",
    "svm= OneVsRestClassifier(LinearSVC(C=.50,)) # C is the regularization param, the large C, the smaller margin of error of the decision boundary line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785714639663\n",
      "[ 0.78255297  0.78867185  0.78591911]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy on random samples of the all the training data:\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = cross_val_score(svm, X_train_features, dev_train_label) # this creates 3 folds and fits the X_train after chi\n",
    "print np.mean(scores)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REAL TEST SET that is used to score in Kaggle submission\n",
    "svm.fit(X_train_features, dev_train_label) #first need to fit the svm model to all train data, pipeline doesnt work\n",
    "real_test = vec.transform(test_data)\n",
    "X_test_real = ch2.transform(real_test) \n",
    "real_predicted = svm.predict(X_test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write TEST DATA to csv for submission\n",
    "header = ['id','cuisine']\n",
    "with open('saru_test_submission.csv', 'w') as fp:\n",
    "    writer = csv.writer(fp, delimiter=',')\n",
    "    # First write the header\n",
    "    writer.writerow(header)\n",
    "    for test_index in range(len(test_data)):\n",
    "        writer.writerow([test_ids[test_index], real_predicted[test_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'adobo corn', u'adobo dri', u'adobo egg', u'adobo extra', u'adobo garlic', u'adobo granul', u'adobo grate', u'adobo green', u'adobo ground', u'adobo jack', u'adobo jalapeno', u'adobo ketchup', u'adobo kidney', u'adobo knorr', u'adobo kosher', u'adobo larg', u'adobo lime', u'adobo masa', u'adobo mayonais', u'adobo oliv']\n"
     ]
    }
   ],
   "source": [
    "# Print out vocab and if you want to output vocab file change is_print to 1\n",
    "vocab= list(np.asarray(vec.get_feature_names())[ch2.get_support()])\n",
    "print vocab[80:100]\n",
    "is_print =0\n",
    "if is_print == 1:\n",
    "    f = open('vocab-saru.txt', 'w')\n",
    "    for item in vocab:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
