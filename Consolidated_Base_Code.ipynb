{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>W207 Final Project</h2>\n",
    "\n",
    "Team Pacific Knights<br/>\n",
    "Members: Alan Wang, Daniel Sheinin, Kuan Lin, Michael Andrew Kennedy, Saru Mehta\n",
    "\n",
    "Competition Description:<br/>\n",
    "“What’s Cooking” - https://www.kaggle.com/c/whats-cooking\n",
    "The goal of this competition is to successfully classify a set of recipes into one of twenty geographic regions of origin according to the ingredients they use. The competition provides a labeled training data set containing lists of raw ingredients and the cuisine they belong to. A second unlabeled data set is provided for scoring purposes.  Competitors are ranked by their ability to accurately label the test set. Competitors are granted up to 5 scoring attempts per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train.json') as data_file:    \n",
    "    all_train_data = json.load(data_file)\n",
    "    all_train_labels = np.array([d[\"cuisine\"] for d in all_train_data])\n",
    "with open('test.json') as data_file:\n",
    "    all_test_data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Exploration:\n",
    "We start by treating the ingredients as bag of words and use text-classification models.\n",
    "Using basic text preprocessor on TD-IDF vectorizer, perform grid search and compare simple models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing texts...\n",
      "grid search on logistic regression:\n",
      "best parameters:\n",
      "{'penalty': 'l2', 'C': 11.0}\n",
      "accuracy: 0.7859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.py:370: ChangedBehaviorWarning: The long-standing behavior to use the estimator's score function in GridSearchCV.score has changed. The scoring parameter is now used.\n",
      "  ChangedBehaviorWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:607: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "grid search on MultinomialNB:\n",
      "best parameters:\n",
      "{'alpha': 0.01}\n",
      "accuracy: 0.7385\n"
     ]
    }
   ],
   "source": [
    "dev_test_data = ['\\n'.join(d[\"ingredients\"]) for d in all_train_data[:len(all_train_data)/3]]\n",
    "dev_test_label = [d[\"cuisine\"] for d in all_train_data[:len(all_train_data)/3]]\n",
    "dev_train_data = ['\\n'.join(d[\"ingredients\"]) for d in all_train_data[len(all_train_data)/3:]]\n",
    "dev_train_label = [d[\"cuisine\"] for d in all_train_data[len(all_train_data)/3:]]\n",
    "\n",
    "def text_preprocessor(s):\n",
    "    return s.lower().replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "\n",
    "print \"vectorizing texts...\"\n",
    "vec = TfidfVectorizer(preprocessor=text_preprocessor, ngram_range=(1,2), max_df=0.5, strip_accents='unicode')\n",
    "dev_train_vec = vec.fit_transform(dev_train_data)\n",
    "dev_test_vec = vec.transform(dev_test_data)\n",
    "\n",
    "print \"grid search on logistic regression:\"\n",
    "params = {'C': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5, 2.0, 5.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0], 'penalty': ['l1', 'l2']}\n",
    "model_logistic = GridSearchCV(LogisticRegression(), param_grid=params, scoring='accuracy')\n",
    "model_logistic.fit(dev_train_vec, dev_train_label)\n",
    "print \"best parameters:\"\n",
    "print str(model_logistic.best_params_)\n",
    "print \"accuracy: %.4f\" % model_logistic.score(dev_test_vec, dev_test_label)\n",
    "print\n",
    "print \"grid search on MultinomialNB:\"\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0]}\n",
    "model_MNB = GridSearchCV(MultinomialNB(), param_grid=alphas)\n",
    "model_MNB.fit(dev_train_vec, dev_train_label)\n",
    "print \"best parameters:\"\n",
    "print str(model_MNB.best_params_)\n",
    "print \"accuracy: %.4f\" % model_MNB.score(dev_test_vec, dev_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take a look at the confusion matrix to see which cousines the basic model is having trouble to classify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make a confusion matrix on the best initial model\n",
      "  84    0    2    0    3    6    0    4    0   13    0    0    0   23    0    2   16    5    6    0 \n",
      "   0  105    3    1    1   44    3    7   19   21    5    1    0    6    0    4   56    1    1    0 \n",
      "   1    2  379    0    1   29    0    2    1   27    1    0    0   18    1    4   73    2    0    0 \n",
      "   1    3    3  769    6    2    0    7    0   14    0   15   16    4    0    2   17    1   23   12 \n",
      "   2    3    1   31  153    6    0    2    0   10    1    2    1   11    1    0   14    2    3    6 \n",
      "   0    4    6    1    3  565    6    7    8  166    0    2    0    8    2    8   65   16    0    1 \n",
      "   0    0    0    2    0    9  272    9    1   65    0    1    0    3   12    1    4    8    0    0 \n",
      "   2    0    0    2    3    4    9  882    0    6    1    0    1   21   15    0   11    0    8    0 \n",
      "   1   12    0    1    0   31    3    1   97   15    3    0    0    4    2    1   34    3    0    0 \n",
      "   1    6    4    0    0  115   39    3    3 2324    0    4    1   20    5    8   59   14    1    0 \n",
      "   3    1    2    1    2    1    0    8    3    2  125    3    0   12    0    0   12    0    0    0 \n",
      "   1    2    1   55    2    7    0   44    1    6    0  326    8    3    0    1   12    0    5    2 \n",
      "   0    1    0   40    1    1    0    0    0    3    0    7  226    0    0    0    4    2    4    3 \n",
      "   4    2    3    8    3   26    5    6    0   43    4    2    0 1983    0    1   49   15    1    0 \n",
      "   0    0    0    0    0    5    9   27    1   12    2    2    0    7  179    1    3    6    1    0 \n",
      "   0    5    1    0    4   24    4    2    7   21    0    0    0    7    0   63   23    1    0    0 \n",
      "   3   21   71    5    2   42    3   11   10   60    8    3    0   37    4    3 1177    6    1    1 \n",
      "   0    1    5    2    1   30    3    3    0   71    0    0    0   33    5    0   21  158    0    0 \n",
      "   6    0    0   25    5    1    1   22    0    2    2    2    3    8    0    0    7    0  396   32 \n",
      "   5    0    0   25    5    2    1    4    0    5    0    1    3    2    1    0    3    1   54  156 \n",
      "\n",
      "classes:\n",
      "[u'brazilian' u'british' u'cajun_creole' u'chinese' u'filipino' u'french'\n",
      " u'greek' u'indian' u'irish' u'italian' u'jamaican' u'japanese' u'korean'\n",
      " u'mexican' u'moroccan' u'russian' u'southern_us' u'spanish' u'thai'\n",
      " u'vietnamese']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "\n",
    "print \"make a confusion matrix on the best initial model\"\n",
    "cm = confusion_matrix(dev_test_label, model_logistic.predict(dev_test_vec))\n",
    "for row in cm:\n",
    "    for col in row: sys.stdout.write(\"%4d \"%col)\n",
    "    sys.stdout.write(\"\\n\")\n",
    "print\n",
    "print \"classes:\"\n",
    "print str(model_logistic.best_estimator_.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Italian and French cuisines are confusing.  Next we will try to preprocess the ingredient list to see if can reduce the confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredient List preprocessing\n",
    "\n",
    "To refine the basic model:\n",
    "- Custom stop words. The basic idea is to use a list of words that are commonly used in ingredients but have no significance to labelling. Results are mixed. I'm able to increase the score by adding certain words, but others decrease the score. The result changes if I re-slice the data. This tells me that I'm probably overfitting. With more work we might be able to come up with a generally useful list.\n",
    "- Stemming: NLTK snowball. Marginal benefit, if any.\n",
    "- n-grams: Some success here with 2-grams. The idea is that creating n-grams from the structured ingredient allows us to be smarter about it than a tokenizer with a bag of words would be. In particular, it allows us to restrict n-grams to within ingredients, rather than spanning ingredients, which assigns meaning to the ingredient order. There is also some improvement from extracting n-grams from the end of the ingredient and not the beginning, since ingredients with more than two words tend to start with less useful adjectives and end with more useful adjective/noun pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "#stop_words = ['chopped', 'ground', 'dark', 'large', 'Italian', 'grated', 'sliced', 'salt', 'kosher',\n",
    "# 'fresh', 'whole', 'minced', 'shredded']\n",
    "\n",
    "# A baseline ingredient processing function that does nothing (for comparison)\n",
    "def proc_ingredients_base(strings):\n",
    "    return strings\n",
    "\n",
    "# The ingredient processing function\n",
    "def proc_ingredients(strings):\n",
    "    # An empty list for the processed ingredients\n",
    "    new_strings = []\n",
    "    \n",
    "    # 2-grams are useful, but any more than 2 degrades performance\n",
    "    ngram_max = 2\n",
    "    \n",
    "    # Loop through ingredients\n",
    "    for s in strings:\n",
    "        # If we're going to use a stemmer and stop words, don't put the stemmer here, because\n",
    "        # if it stems our stop words they won't be removed\n",
    "        #s = stemmer.stem(s)\n",
    "        \n",
    "        # split ingredient into words and add words to list\n",
    "        # (here's a better place to use the stemmer)\n",
    "        #tokens = [stemmer.stem(t) for t in s.split() if t not in stop_words]\n",
    "        tokens = [t for t in s.split() if t not in stop_words]\n",
    "        \n",
    "        # Add each individual token to the list\n",
    "        new_strings.extend(tokens)\n",
    "        \n",
    "        # simulate n-grams (by concatenating words without spaces) within each ingredient\n",
    "        n_original_tokens = len(tokens)\n",
    "        if n_original_tokens > 1:\n",
    "            for n in xrange(2, min(ngram_max + 1, n_original_tokens + 1)):\n",
    "                # This takes all valid n-grams within the ingredient...\n",
    "                #for i in xrange(n_original_tokens - n + 1):\n",
    "                # This just takes the n-gram at the end of the ingredient...\n",
    "                for i in xrange(max(0, n_original_tokens - n), n_original_tokens - n + 1):\n",
    "                    new_strings.append(\"\".join(tokens[i: i + n]))\n",
    "                    \n",
    "    return new_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Pipeline\n",
    "\n",
    "Combine bag-of-words with other features extracted from the ingredient lists and pipe into a single classifier. Code loosely adapted from http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract all of the features we want from the recipe data\"\"\"\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, recipes):\n",
    "        features = np.recarray(shape=(len(recipes),),\n",
    "                               dtype=[('ingredient_text', object),\n",
    "                                      ('ingredient_count', np.int32, (1,)),\n",
    "                                      ('character_count', np.int32, (1,)),\n",
    "                                      ('avg_word_length', np.float64, (1,)),\n",
    "                                      ('avg_ing_length', np.float64, (1,)),\n",
    "                                     ])\n",
    "        for i, recipe in enumerate(recipes):\n",
    "            ingredients = recipe['ingredients']\n",
    "            features['ingredient_count'][i] = len(ingredients)\n",
    "            words = ' '.join(ingredients)\n",
    "            char_count = len(''.join(words.split()))\n",
    "            features['character_count'][i] = char_count\n",
    "            features['avg_word_length'] = float(char_count) / len(words.split())\n",
    "            features['avg_ing_length'] = float(len(words.split())) / len(ingredients)\n",
    "            features['ingredient_text'][i] = ' '.join(proc_ingredients(ingredients))\n",
    "        return features\n",
    "    \n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\"\"\"\n",
    "    \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]    \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('extract', FeatureExtractor()),\n",
    "    \n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            \n",
    "            ('text', Pipeline([\n",
    "                ('select', ItemSelector(key='ingredient_text')),\n",
    "                ('tfidf', TfidfVectorizer(max_df=0.5)),\n",
    "            ])),         \n",
    "\n",
    "            ('ingredient_count', Pipeline([\n",
    "                ('select', ItemSelector(key='ingredient_count')),\n",
    "            ])),\n",
    "                    \n",
    "            ('character_count', Pipeline([\n",
    "                ('select', ItemSelector(key='character_count')),\n",
    "            ])),                    \n",
    "\n",
    "            ('avg_word_length', Pipeline([\n",
    "                ('select', ItemSelector(key='avg_word_length')),\n",
    "            ])),\n",
    "                    \n",
    "            ('avg_ing_length', Pipeline([\n",
    "                ('select', ItemSelector(key='avg_ing_length')),\n",
    "            ])),\n",
    "        ],\n",
    "                \n",
    "        # playing with the weights doesn't seem to help when regression is the classifier\n",
    "        transformer_weights={\n",
    "            'text': 1.0,\n",
    "            'ingredient_count': 1.0,\n",
    "            'character_count': 1.0,\n",
    "            'avg_word_length': 1.0,\n",
    "            'avg_ing_length': 1.0,\n",
    "        },\n",
    "    )),\n",
    "\n",
    "    #('regression', GridSearchCV(LogisticRegression(), {'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]})),\n",
    "    ('regression', LogisticRegression(C=10.0)),\n",
    "    #('svc', SVC(kernel='linear')),\n",
    "    #('svc', LinearSVC(penalty=\"l1\", dual=False, tol=1e-3)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing: generate pipeline score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.793691909188\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, train_labels, dev_labels = train_test_split(all_train_data, \n",
    "                                                                  all_train_labels, \n",
    "                                                                  test_size=0.33, \n",
    "                                                                  random_state=12)\n",
    "\n",
    "pipeline.fit(train_data, train_labels)\n",
    "print pipeline.score(dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results\n",
    "\n",
    "Calculate R value for each prediction and have a look at the mistakes. A couple of potential insights:\n",
    "- The worst mistakes are pretty hopeless. Some of theme are non-descript, but a lot of them use the \"wrong\" ingredients for the label, e.g. Parmesan cheese in Chinese food. These are not necessarily authentic recipes--they are (probably Western) people's contributed interpretations of some type of cuisine, so they're bound to be weird and overlap.\n",
    "- Some of the near misses have ingredients with more than 2 words in which all but the last couple of words are toss-aways, e.g. \"ground black pepper\", \"hot pepper sauce\", \"light corn syrop\", \"grated lemon zest\". Including some of these words in a stop word list is one way to go, but the general structure of the ingredients seems to be more important words towards the end. Hence the modification to n-gram construction above in which we just take the 2-gram from the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst predictions\n",
      "###############################\n",
      "\n",
      "742.00695089\n",
      "Predicted: british\n",
      "Actual: french\n",
      "\n",
      "[u'sugar', u'honey', u'buckwheat honey', u'garbanzo bean flour', u'flaxseed', u'sweet rice flour', u'pectin', u'tapioca flour', u'dry yeast', u'poppy seeds', u'buckwheat flour', u'double-acting baking powder', u'sponge', u'water', u'teff', u'sea salt', u'brown rice flour', u'psyllium husks', u'sunflower seeds', u'millet', u'grapeseed oil', u'gluten-free oat', u'oil']\n",
      "______________________________ \n",
      "\n",
      "782.778091723\n",
      "Predicted: southern_us\n",
      "Actual: spanish\n",
      "\n",
      "[u'sugar', u'unsalted butter', u'milk', u'all-purpose flour', u'warm water', u'salt', u'active dry yeast', u'white cornmeal']\n",
      "______________________________ \n",
      "\n",
      "791.508030796\n",
      "Predicted: mexican\n",
      "Actual: filipino\n",
      "\n",
      "[u'onion powder', u'ground cumin', u'ground black pepper', u'salt', u'garlic powder', u'paprika', u'chili powder', u'dried oregano']\n",
      "______________________________ \n",
      "\n",
      "1075.63220963\n",
      "Predicted: chinese\n",
      "Actual: japanese\n",
      "\n",
      "[u'hoisin sauce', u'corn starch', u'green onions', u'extra firm tofu', u'oil']\n",
      "______________________________ \n",
      "\n",
      "1026.7642806\n",
      "Predicted: chinese\n",
      "Actual: filipino\n",
      "\n",
      "[u'celery ribs', u'kosher salt', u'leeks', u'cilantro', u'chinese five-spice powder', u'calamansi', u'celery leaves', u'orange', u'peeled fresh ginger', u'peanut oil', u'carrots', u'chinese chili paste', u'scallion greens', u'lemongrass', u'bay leaves', u'pineapple juice', u'garlic cloves', u'serrano chile', u'pork belly', u'sherry vinegar', u'shallots', u'freshly ground pepper', u'ground white pepper', u'orange zest']\n",
      "______________________________ \n",
      "\n",
      "923.225287545\n",
      "Predicted: cajun_creole\n",
      "Actual: british\n",
      "\n",
      "[u'shallots', u'cayenne pepper', u'mace', u'butter', u'bay leaf', u'fresh lemon', u'salt', u'black pepper', u'Tabasco Pepper Sauce', u'shrimp']\n",
      "______________________________ \n",
      "\n",
      "821.292531823\n",
      "Predicted: italian\n",
      "Actual: thai\n",
      "\n",
      "[u'water', u'part-skim mozzarella cheese', u'garlic cloves', u'fresh basil', u'eggplant', u'golden raisins', u'olive oil', u'grated parmesan cheese', u'fresh lemon juice', u'black pepper', u'pitas', u'salt']\n",
      "______________________________ \n",
      "\n",
      "1199.1854257\n",
      "Predicted: southern_us\n",
      "Actual: jamaican\n",
      "\n",
      "[u'mayonaise', u'paprika', u'prepared mustard', u'pepper', u'salt', u'eggs', u'yellow mustard']\n",
      "______________________________ \n",
      "\n",
      "1112.53359958\n",
      "Predicted: italian\n",
      "Actual: greek\n",
      "\n",
      "[u'swordfish steaks', u'fresh basil', u'fresh lime juice', u'minced garlic', u'tomatoes', u'butter']\n",
      "______________________________ \n",
      "\n",
      "921.47781043\n",
      "Predicted: chinese\n",
      "Actual: filipino\n",
      "\n",
      "[u'large egg whites', u'sesame oil', u'peanut oil', u'soy sauce', u'peeled fresh ginger', u'ground pork', u'corn starch', u'minced garlic', u'jicama', u'salt', u'white sesame seeds', u'sugar', u'black sesame seeds', u'wonton wrappers', u'scallions']\n",
      "______________________________ \n",
      "\n",
      "872.230616126\n",
      "Predicted: spanish\n",
      "Actual: korean\n",
      "\n",
      "[u'vegetable oil', u'salt', u'green bell pepper', u'red bell pepper', u'large eggs']\n",
      "______________________________ \n",
      "\n",
      "1066.87467889\n",
      "Predicted: italian\n",
      "Actual: southern_us\n",
      "\n",
      "[u'sliced salami', u'grated parmesan cheese', u'sliced ham', u'dried basil', u'pitted green olives', u'dried oregano', u'mozzarella cheese', u'oil-cured black olives', u'Italian bread', u'olive oil', u'provolone cheese']\n",
      "______________________________ \n",
      "\n",
      "1246.53889735\n",
      "Predicted: french\n",
      "Actual: jamaican\n",
      "\n",
      "[u'gruyere cheese', u'boneless skinless chicken breasts', u'carrots', u'zucchini', u'ham', u'vegetable oil']\n",
      "______________________________ \n",
      "\n",
      "1624.65648543\n",
      "Predicted: vietnamese\n",
      "Actual: cajun_creole\n",
      "\n",
      "[u'water', u'coffee']\n",
      "______________________________ \n",
      "\n",
      "4741.45732957\n",
      "Predicted: french\n",
      "Actual: korean\n",
      "\n",
      "[u'kosher salt', u'sea scallops', u'baby arugula', u'all-purpose flour', u'white pepper', u'sauerkraut', u'unsalted butter', u'heavy cream', u'smoked paprika', u'white onion', u'olive oil', u'dijon mustard', u'bacon', u'sour cream', u'chicken stock', u'sumac', u'cayenne', u'dry white wine', u'ground coriander']\n",
      "______________________________ \n",
      "\n",
      "13027.1559225\n",
      "Predicted: italian\n",
      "Actual: moroccan\n",
      "\n",
      "[u'tomato sauce', u'elbow macaroni', u'pasta sauce', u'broccoli', u'cauliflower', u'grated parmesan cheese', u'part-skim mozzarella', u'whipping cream']\n",
      "______________________________ \n",
      "\n",
      "14187.0457257\n",
      "Predicted: italian\n",
      "Actual: korean\n",
      "\n",
      "[u'fresh rosemary', u'balsamic vinegar', u'olive oil', u'pepper', u'salt', u'fresh thyme leaves']\n",
      "______________________________ \n",
      "\n",
      "3448.43156279\n",
      "Predicted: italian\n",
      "Actual: chinese\n",
      "\n",
      "[u'pasta', u'parmesan cheese', u'garlic', u'sour cream', u'pepper', u'red pepper flakes', u'yellow onion', u'chipotles in adobo', u'tomato sauce', u'boneless skinless chicken breasts', u'salt', u'dried red chile peppers', u'olive oil', u'heavy cream', u'shrimp', u'italian seasoning']\n",
      "______________________________ \n",
      "\n",
      "1266.09497365\n",
      "Predicted: spanish\n",
      "Actual: russian\n",
      "\n",
      "[u'kale', u'crushed red pepper flakes', u'kosher salt', u'russet potatoes', u'onions', u'reduced sodium chicken broth', u'olive oil', u'spanish chorizo', u'pepper', u'large garlic cloves']\n",
      "______________________________ \n",
      "\n",
      "1666.58640682\n",
      "Predicted: french\n",
      "Actual: moroccan\n",
      "\n",
      "[u'dry white wine', u'navy beans', u'onions', u'cooking spray', u'mussels', u'salt']\n",
      "______________________________ \n",
      "\n",
      "17149.5686221\n",
      "Predicted: cajun_creole\n",
      "Actual: filipino\n",
      "\n",
      "[u'sausage links', u'ground black pepper', u'green onions', u'salt', u'okra', u'water', u'chopped green bell pepper', u'diced tomatoes', u'cayenne pepper', u'fresh parsley', u'minced garlic', u'unsalted butter', u'bacon', u'all-purpose flour', u'uncook medium shrimp, peel and devein', u'dried thyme', u'bay leaves', u'chopped celery', u'chopped onion']\n",
      "______________________________ \n",
      "\n",
      "6118.78164449\n",
      "Predicted: italian\n",
      "Actual: russian\n",
      "\n",
      "[u'fresh rosemary', u'freshly grated parmesan', u'red russian kale', u'onions', u'minced garlic', u'Vegeta Seasoning', u'chickpeas', u'tomato sauce', u'ground black pepper', u'tuscan sausage', u'chicken stock', u'olive oil', u'tomatoes with juice', u'chopped fresh sage']\n",
      "______________________________ \n",
      "\n",
      "2359.01457308\n",
      "Predicted: mexican\n",
      "Actual: filipino\n",
      "\n",
      "[u'sugar', u'fresh cilantro', u'hoisin sauce', u'butter', u'crushed red pepper flakes', u'strawberries', u'ground coriander', u'red bell pepper', u'adobo sauce', u'green peppercorns', u'kosher salt', u'salted butter', u'chives', u'sea salt', u'extra-virgin olive oil', u'taco seasoning', u'pork loin chops', u'chipotle peppers', u'chorizo sausage', u'blueberri preserv', u'orange', u'ground black pepper', u'balsamic vinegar', u'basil', u'garlic', u'filet', u'thyme leaves', u'chipotles in adobo', u'unsweetened cocoa powder', u'black pepper', u'honey', u'boneless skinless chicken breasts', u'heavy cream', u'cracked black pepper', u'coffee beans', u'garlic cloves', u'steak', u'onions']\n",
      "______________________________ \n",
      "\n",
      "2250.18642182\n",
      "Predicted: vietnamese\n",
      "Actual: greek\n",
      "\n",
      "[u'coffee', u'sugar']\n",
      "______________________________ \n",
      "\n",
      "23593.6114779\n",
      "Predicted: mexican\n",
      "Actual: french\n",
      "\n",
      "[u'tomatoes', u'flour', u'salt', u'dried oregano', u'white vinegar', u'garlic powder', u'paprika', u'sweetened condensed milk', u'black pepper', u'lean ground beef', u'onions', u'pita bread', u'onion powder', u'cayenne pepper']\n",
      "______________________________ \n",
      "\n",
      "8193.38065253\n",
      "Predicted: mexican\n",
      "Actual: chinese\n",
      "\n",
      "[u'suckling pig', u'bay leaves', u'worcestershire sauce', u'salt', u'lard', u'white onion', u'jalapeno chilies', u'raisins', u'red bliss potato', u'carrots', u'green apples', u'tomatoes', u'ground black pepper', u'Mexican oregano', u'fresh orange juice', u'pimento stuffed olives', u'fresh lime juice', u'adobo', u'fresh thyme', u'cinnamon', u'garlic', u'juice', u'fresh pineapple']\n",
      "______________________________ \n",
      "\n",
      "1479.51233807\n",
      "Predicted: thai\n",
      "Actual: japanese\n",
      "\n",
      "[u'lychees', u'winesap', u'chili powder', u'unsalted dry roast peanuts', u'asian fish sauce', u'golden delicious apples', u'scallions', u'cayenne', u'bacon', u'fresh lime juice']\n",
      "______________________________ \n",
      "\n",
      "2031.1162334\n",
      "Predicted: thai\n",
      "Actual: korean\n",
      "\n",
      "[u'mayonaise', u'salt', u'chopped cilantro fresh', u'pepper', u'peanut oil', u'bread crumb fresh', u'crabmeat', u'asian fish sauce', u'fresh ginger', u'shrimp']\n",
      "______________________________ \n",
      "\n",
      "1385.7699379\n",
      "Predicted: japanese\n",
      "Actual: korean\n",
      "\n",
      "[u'granulated sugar', u'club soda', u'pink grapefruit juice', u'sake']\n",
      "______________________________ \n",
      "\n",
      "3131.57321787\n",
      "Predicted: indian\n",
      "Actual: russian\n",
      "\n",
      "[u'sugar', u'cardamom pods', u'fresh ginger', u'fresh mint', u'clove', u'lemon zest', u'honey', u'cinnamon sticks']\n",
      "______________________________ \n",
      "\n",
      "Nearest misses\n",
      "###############################\n",
      "\n",
      "1.00762350654\n",
      "Predicted: italian\n",
      "Actual: spanish\n",
      "\n",
      "[u'hazelnuts', u'cubed bread', u'blanched almonds', u'chili', u'large garlic cloves', u'roast red peppers, drain', u'kosher salt', u'red wine vinegar', u'country bread', u'olive oil', u'extra-virgin olive oil', u'plum tomatoes']\n",
      "______________________________ \n",
      "\n",
      "1.00755714508\n",
      "Predicted: italian\n",
      "Actual: british\n",
      "\n",
      "[u'milk', u'salt', u'instant yeast', u'bread flour', u'sugar', u'large eggs', u'semolina', u'softened butter']\n",
      "______________________________ \n",
      "\n",
      "1.00205198443\n",
      "Predicted: french\n",
      "Actual: italian\n",
      "\n",
      "[u'unsalted butter', u'baking powder', u'bittersweet chocolate', u'raw pistachios', u'dark rum', u'salt', u'sugar', u'large eggs', u'cinnamon', u'unsweetened cocoa powder', u'large egg whites', u'vanilla bean seeds', u'all-purpose flour']\n",
      "______________________________ \n",
      "\n",
      "1.00858224534\n",
      "Predicted: chinese\n",
      "Actual: southern_us\n",
      "\n",
      "[u'whole cloves', u'frozen orange juice concentrate', u'honey', u'boneless ham']\n",
      "______________________________ \n",
      "\n",
      "1.00295960881\n",
      "Predicted: greek\n",
      "Actual: japanese\n",
      "\n",
      "[u'salt', u'sweet onion', u'toasted sesame seeds', u'fresh dill', u'rice vinegar', u'japanese cucumber']\n",
      "______________________________ \n",
      "\n",
      "1.00594948639\n",
      "Predicted: southern_us\n",
      "Actual: french\n",
      "\n",
      "[u'powdered sugar', u'unsweetened baking chocolate', u'milk', u'margarine', u'graham crackers', u'chocolate instant pudding', u'frozen whip topping, thaw', u'vanilla instant pudding']\n",
      "______________________________ \n",
      "\n",
      "1.00475304071\n",
      "Predicted: greek\n",
      "Actual: italian\n",
      "\n",
      "[u'green onions', u'rice', u'feta cheese', u'garlic', u'sun-dried tomatoes in oil', u'olive oil', u'spices', u'toasted pine nuts', u'bell pepper', u'white beans']\n",
      "______________________________ \n",
      "\n",
      "1.00452853694\n",
      "Predicted: southern_us\n",
      "Actual: cajun_creole\n",
      "\n",
      "[u'chicken broth', u'ground red pepper', u'all-purpose flour', u'celery ribs', u'green onions', u'salt', u'grits', u'chopped green bell pepper', u'heavy cream', u'onions', u'andouille sausage', u'butter', u'shrimp']\n",
      "______________________________ \n",
      "\n",
      "1.00706174967\n",
      "Predicted: british\n",
      "Actual: french\n",
      "\n",
      "[u'warm water', u'salt', u'unsalted butter', u'caster sugar', u'bread flour', u'instant yeast']\n",
      "______________________________ \n",
      "\n",
      "1.00201583665\n",
      "Predicted: indian\n",
      "Actual: southern_us\n",
      "\n",
      "[u'curry leaves', u'pepper', u'bacon', u'salt', u'red bell pepper', u'cooked rice', u'black-eyed peas', u'ginger', u'cumin seed', u'ground turmeric', u'collard greens', u'curry powder', u'crushed red pepper flakes', u'green chilies', u'onions', u'chicken stock', u'pork', u'cinnamon', u'garlic', u'black mustard seeds']\n",
      "______________________________ \n",
      "\n",
      "1.00656462778\n",
      "Predicted: greek\n",
      "Actual: italian\n",
      "\n",
      "[u'large egg whites', u'large garlic cloves', u'onions', u'pepper', u'butter', u'whole wheat thin spaghetti', u'frozen chopped spinach', u'fat free milk', u'salt', u'tomato sauce', u'large eggs', u'feta cheese crumbles']\n",
      "______________________________ \n",
      "\n",
      "1.00749427079\n",
      "Predicted: moroccan\n",
      "Actual: mexican\n",
      "\n",
      "[u'pie dough', u'olive oil', u'golden raisins', u'pimento stuffed olives', u'jalapeno chilies', u'chopped onion', u'fresh lime juice', u'ground cinnamon', u'chopped green bell pepper', u'recipe crumbles', u'garlic cloves', u'sliced almonds', u'cooking spray', u'ground allspice', u'ground cumin']\n",
      "______________________________ \n",
      "\n",
      "1.00683801279\n",
      "Predicted: mexican\n",
      "Actual: southern_us\n",
      "\n",
      "[u'pepper', u'pinto beans', u'salt', u'water', u'onions', u'beans', u'meat bones']\n",
      "______________________________ \n",
      "\n",
      "1.00923783874\n",
      "Predicted: british\n",
      "Actual: southern_us\n",
      "\n",
      "[u'pepper', u'all-purpose flour', u'top round steak', u'flour', u'oil', u'eggs', u'milk', u'cayenne pepper', u'kosher salt', u'salt']\n",
      "______________________________ \n",
      "\n",
      "1.01481692692\n",
      "Predicted: thai\n",
      "Actual: vietnamese\n",
      "\n",
      "[u'chili paste', u'minced garlic', u'lime juice', u'sugar', u'asian fish sauce']\n",
      "______________________________ \n",
      "\n",
      "1.01620734758\n",
      "Predicted: southern_us\n",
      "Actual: french\n",
      "\n",
      "[u'sugar', u'all-purpose flour', u'butter', u'whole milk', u'eggs', u'vanilla extract']\n",
      "______________________________ \n",
      "\n",
      "1.00982864653\n",
      "Predicted: jamaican\n",
      "Actual: chinese\n",
      "\n",
      "[u'ground ginger', u'minced garlic', u'ground nutmeg', u'green onions', u'salt', u'soy sauce', u'olive oil', u'boneless chicken breast', u'ground thyme', u'eggs', u'honey', u'ground sage', u'vegetable oil', u'cayenne pepper', u'black pepper', u'ground pepper', u'flour', u'paprika']\n",
      "______________________________ \n",
      "\n",
      "1.00924924618\n",
      "Predicted: chinese\n",
      "Actual: filipino\n",
      "\n",
      "[u'white vinegar', u'water', u'brown rice', u'canola oil', u'black peppercorns', u'Kikkoman Soy Sauce', u'corn starch', u'cold water', u'sunchokes', u'okra pods', u'minced garlic', u'bay leaves', u'chicken thighs']\n",
      "______________________________ \n",
      "\n",
      "1.01134758807\n",
      "Predicted: british\n",
      "Actual: southern_us\n",
      "\n",
      "[u'ground cinnamon', u'baking powder', u'free range egg', u'ground ginger', u'ground cloves', u'salt', u'powdered sugar', u'vegetable oil', u'dark brown sugar', u'plain flour', u'unsalted butter', u'grated nutmeg']\n",
      "______________________________ \n",
      "\n",
      "1.01022008704\n",
      "Predicted: greek\n",
      "Actual: moroccan\n",
      "\n",
      "[u'dried lentils', u'fresh cilantro', u'vegetable stock', u'yellow onion', u'flat leaf parsley', u'fresh dill', u'kidney beans', u'garlic', u'chickpeas', u'plain yogurt', u'egg noodles', u'salt', u'fresh mint', u'fava beans', u'olive oil', u'dark leafy greens', u'lima beans', u'ground turmeric']\n",
      "______________________________ \n",
      "\n",
      "1.01635541738\n",
      "Predicted: italian\n",
      "Actual: irish\n",
      "\n",
      "[u'egg whites', u'butter', u'grated lemon zest', u'light brown sugar', u'golden raisins', u'salt', u'whiskey', u'egg yolks', u'lemon', u'confectioners sugar', u'ground cloves', u'baking powder', u'all-purpose flour']\n",
      "______________________________ \n",
      "\n",
      "1.01473192788\n",
      "Predicted: french\n",
      "Actual: italian\n",
      "\n",
      "[u'sugar', u'whipping cream', u'grated orange peel', u'dry white wine', u'dried cranberries', u'crystallized ginger', u'calimyrna figs', u'large egg yolks', u'sauce']\n",
      "______________________________ \n",
      "\n",
      "1.01851474319\n",
      "Predicted: italian\n",
      "Actual: southern_us\n",
      "\n",
      "[u'baking powder', u'salt', u\"I Can't Believe It's Not Butter!\\xae Spread\", u'milk', u'all-purpose flour']\n",
      "______________________________ \n",
      "\n",
      "1.02149357129\n",
      "Predicted: southern_us\n",
      "Actual: french\n",
      "\n",
      "[u'vegetable oil', u'garlic', u'dried thyme', u'paprika', u'salt', u'ground black pepper', u'dry mustard', u'dried tarragon leaves', u'worcestershire sauce', u'white wine vinegar']\n",
      "______________________________ \n",
      "\n",
      "1.01930560065\n",
      "Predicted: mexican\n",
      "Actual: indian\n",
      "\n",
      "[u'vegetable oil', u'onions', u'tomatoes', u'chickpeas', u'chile powder', u'salt', u'ground cumin', u'fresh cilantro', u'cumin seed']\n",
      "______________________________ \n",
      "\n",
      "1.01859515698\n",
      "Predicted: korean\n",
      "Actual: chinese\n",
      "\n",
      "[u'vegetable oil', u'green onions', u'all-purpose flour', u'water', u'fine sea salt', u'sesame oil']\n",
      "______________________________ \n",
      "\n",
      "1.02183964694\n",
      "Predicted: southern_us\n",
      "Actual: chinese\n",
      "\n",
      "[u'corn', u'corn starch', u'chicken stock', u'cream style corn', u'sliced chicken', u'pepper', u'sesame oil', u'water', u'salt']\n",
      "______________________________ \n",
      "\n",
      "1.0248978742\n",
      "Predicted: french\n",
      "Actual: southern_us\n",
      "\n",
      "[u'large eggs', u'salt', u'grated orange', u'powdered sugar', u'butter', u'fresh lemon juice', u'baking powder', u'all-purpose flour', u'granulated sugar', u'fresh orange juice', u'orange rind']\n",
      "______________________________ \n",
      "\n",
      "1.02734760856\n",
      "Predicted: french\n",
      "Actual: southern_us\n",
      "\n",
      "[u'refrigerated piecrusts', u'chopped walnuts', u'eggs', u'all-purpose flour', u'vanilla', u'chocolate morsels', u'sugar', u'margarine']\n",
      "______________________________ \n",
      "\n",
      "1.02317116466\n",
      "Predicted: italian\n",
      "Actual: southern_us\n",
      "\n",
      "[u'peaches', u'garlic cloves', u'brown sugar', u'salt', u'fresh basil', u'balsamic vinegar', u'olive oil', u'freshly ground pepper']\n",
      "______________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_r(R, indexes, pred, data, labels):\n",
    "    for i in indexes:\n",
    "        print R[i]\n",
    "        print \"Predicted: %s\" % pred[i]\n",
    "        print \"Actual: %s\" % labels[i]\n",
    "        print '\\n',\n",
    "        print data[i]['ingredients']\n",
    "        print '_' * 30, '\\n'         # print separator       \n",
    "        \n",
    "# for testing: worst R values\n",
    "predicted = pipeline.predict(dev_data)\n",
    "probs = pipeline.predict_proba(dev_data)\n",
    "\n",
    "# calculate R for each prediction\n",
    "R = np.empty(probs.shape[0])\n",
    "for i in range(probs.shape[0]):\n",
    "    R[i] = max(probs[i]) / probs[i,np.where(pipeline.classes_ == dev_labels[i])]\n",
    "\n",
    "# Find top 30 R values, i.e. the worst 30 predictions\n",
    "worst_predictions = np.argpartition(R, -30)[-30:]\n",
    "print \"Worst predictions\"\n",
    "print \"###############################\\n\"\n",
    "print_r(R, worst_predictions, predicted, dev_data, dev_labels)\n",
    "\n",
    "# Find 30 with the lowest R values that were not matched correctly\n",
    "R[R == 1] = np.inf\n",
    "nearest_misses = np.argpartition(R, 30)[:30]\n",
    "print \"Nearest misses\"\n",
    "print \"###############################\\n\"\n",
    "print_r(R, nearest_misses, predicted, dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For live run: predict and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(all_train_data, all_train_labels)\n",
    "pred = pipeline.predict(all_test_data)\n",
    "ids = np.array([d[\"id\"] for d in all_test_data])\n",
    "results = np.vstack((ids, pred)).T\n",
    "np.savetxt(\"submission.csv\", results, delimiter=\",\", fmt=\"%s\", header=\"id,cuisine\", comments=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
